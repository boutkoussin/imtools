{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amoeba-based superpixel partitioning of multispectral images into elementary, uniform, connected units\n",
    "---\n",
    "\n",
    "Scene segmentation is a difficult task because of the high complexity of images, where complexity refers to the large variety of pictorial representations of objects with the same semantic meaning and also to the extensive amount of available details. We introduce a new approach inspired by the SLIC methodology that implicitly enforces connectivity and efficiently generates compact, connected, and nearly uniform superpixels. The new formulation will introduce a different neighbourhood structure that implicitly encode spatial proximity and contiguity in it. \n",
    "\n",
    "\n",
    "#### Rationale\n",
    "\n",
    "The problem of segmenting an image into semantically meaningful units is a common one in image processing [[SM00]](#SM00). Indeed, visual information extraction is often regarded as a segmentation issue [[FH04]](#FH04). Scene segmentation captures the local redundancy in the data, by reducing noise and variability, but aggregating pixels into segments often entails a decision that is unrelated to the final task. The goal is then to perform this decision in a conservative way to minimize the risk of merging unrelated pixels from different objects. As a pragmatic alternative, **over-segmentation forms a one-to-many partitioning of scene features into smaller segments of distinct spectra**. It seems indeed natural, and presumably more efficient, to **work with perceptually meaningful entities obtained from low-level grouping processes instead of the pixel representation**. In that context, **superpixels obtained from conservative over-segmentation are a common pre-processing step for recovering image features** [[RM03](#RM03), [LDM07](#LDM07), [MPWMJ08](#MPWMJ08)]. \n",
    "\n",
    "We introduce an algorithm that _(i)_ works essentially like a _kmeans_ based local clustering of pixels, but _(ii)_ enforces connectivity, so that it can efficiently generate compact, connected, and nearly uniform superpixels. This approach is based on the **estimation of amoeba-like neighborhoods around selected cluster centers that exploit the connections between successive image pixels along geodesic paths in the image**. The resulting superpixels capture the spatial/spectral redundancy in images and greatly reduce the complexity of subsequent image processing tasks [[Hanbury08]](#Hanbury08). They provide convenient primitives from which to compute local image features when objects present in the scene have diverse scales or when they are not known in advance [[BBI08]](#BBI08).\n",
    "\n",
    "#### Original SLIC segmentation\n",
    "\n",
    "Whereas the idea of operating on atomic, homogeneously colored or textured regions is not new, the popular term **superpixels** has been coined recently. In practice, any segmentation of an image into contiguous regions could produce superpixels. However, for many image processing tasks, compact and highly uniform superpixels that respect image boundaries are desired.\n",
    "The SLIC algorithm proposed in [[ASSLFS12]](#ASSLFS12) partitions an image into a desired number of regular, compact superpixels with a low computational overhead.\n",
    "It is essentially a **_kmeans_ algorithm that partitions a color image into a desired number of regular, compact superpixels with a low computational overhead**. Namely, it locally clusters pixels in the combined 5D color ($(L, a, b)$ values of the CIELAB color space) and image plane ($(x, y)$ pixel coordinates) space. Note that the reason why CIELAB color space is chosen is that it is perceptually uniform for small color distance.  \n",
    "\n",
    "The SLIC algorithm takes as input a desired number of approximately equally-sized superpixels $K$ and begins by sampling $K$ regularly spaced cluster centers $C_k = [L_k, a_k, b_k, x_k, y_k]^T$ at regular grid intervals $S$ (depending on $K$ and the number $N$ of pixels in the image: $S = \\sqrt{N/K}$). In practice, for roughly equally sized superpixels there would be a superpixel center at every grid interval $S$. Since the spatial extent of any superpixel is approximately $S^2$ (the approximate area of a superpixel), it is assumed that pixels that are associated with this cluster center lie within a $2S \\times 2S$ area around the superpixel center on the $(x, y)$ plane. This becomes the search area for the pixels nearest to each cluster center. Instead of directly using the Euclidean distance in the 5D space, **a _\"perceptual closeness\"_ measure $D_s$ that considers superpixel size is introduced to control the compactness of superpixels** [[ASSLFS12]](#ASSLFS12):\n",
    "$$\n",
    "D_s ({\\mathbf x}_i, {C_k}) = d_{lab} ({\\mathbf x}_i, {C}_k) + \\frac{m}{S} \\cdot {d}_{xy} ({\\mathbf x}_i, {C}_k),\n",
    "\\qquad (1)\n",
    "$$\n",
    "defined as the (weighted) sum of the spectral distance $d_{lab}$ in the color space:\n",
    "$$\n",
    "d_{lab} ({\\mathbf x}_i, {C}_k) = \\sqrt{(L_k - L_i)^2 + (a_k - a_i)^2 + (b_k - b_i)^2} \n",
    "\\qquad (2a)\n",
    "$$\n",
    "and the spatial distance $d_{xy}$ in the plane (normalized by the grid interval $S$):\n",
    "$$\n",
    "d_{xy} ({\\mathbf x}_i, {C_k}) = \\sqrt{(x_k - x_i)^2 + (y_k - y_i)^2} .\n",
    "\\qquad (2b)\n",
    "$$\n",
    "of a pixel ${\\mathbf x}_i=[ L_i, a_i, b_i, x_i, y_i ]^T$ to any cluster center $C_k$. Here, $m$ is introduced to control the compactness of superpixels (the greater the value of $m$, the more spatial proximity is emphasized and the more compact the cluster). \n",
    "**This distance is used to cluster together preceptually close pixels in an iterative manner**. Namely, each pixel ${\\mathbf x}_i$ is associated with the nearest cluster center $C_k$  (_w.r.t_ the distance $D_s$) whose search area overlaps this pixel. After all the pixels are associated with their nearest cluster center, new centers are  computed as the barycenters of all the pixels belonging to the updated clusters. This process is iterated till convergence to produce approximately equally-sized superpixels. \n",
    "\n",
    "A post-processing of the clusters output by the _kmeans_ clustering is necessary to enforce connectivity. Indeed, the distance $D_s$ (based on composite Euclidean distance), while being easy to calculate, **does not take into account the image intensity values between two image pixels and thus ignores connectivity**: a pixel belonging to an object distinct from the cluster center but with similar spectral values, while separated by other intermediate distinct structures, can still reach low $D_s$ and be considered as _\"perceptually close\"_, hence the superpixels can be ripped apart. \n",
    "\n",
    "#### Extended amoeba-based SLIC segmentation\n",
    "\n",
    "Generally, making an appropriate choice of homogeneity criterion is critical to the success of any region-based segmentation procedure, especially for multispectral images, where the criterion is highly dependent on the choice of the (spectral and spatial) closeness measure [[FH04]](#FH04). Our algorithm is based on the **estimation of amoeba-like neighborhoods around the cluster centers** [[LDM07](#LDM07), [GS09a](#GS09)]. **The amoeba construction exploits the connections between successive image pixels along the so-called geodesic paths** to define a related measure [[DP06](#DP06), [Soille08](#Soille08)]. It describes **similarity relationships based on pairwise differences between neighboring pixels** (_e.g._, using the connections between successive image pixels along geodesic paths) [[BM06](#BM06), [GDS10](#GDS10)].\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"img/excerpt1.png\" alt=\"input excerpt\" width=\"250\"></td>\n",
    "<td><img src=\"img/exceprt1-amoeba-superpixels.png\" alt=\"amoeba superpixels\" width=\"250\"> </td>\n",
    "<td><img src=\"img/excerpt1-mean-amoeba-approximations.png\" alt=\"mean amoeba approximations\" width=\"250\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\" width=\"250\">input excerpt</td>\n",
    "<td align=\"center\" width=\"250\">amoeba superpixels (more scales displayed)</td>\n",
    "<td align=\"center\" width=\"250\">amoeba mean-averaged approximations (more scales displayed</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "A new _\"perceptual closeness\"_ measure between ${\\mathbf x}_i$ and $C_k$ is formulated as the minimal length of the  shortest path ${\\cal P} = (p_0={\\mathbf x}_i, p_1, \\cdots, p_n=C_k)$ joining them [[GS08](#GS08), [GS08](#GS09)]. $D_a$ is estimated as the **combined spatial/spectral cost of \"traveling\"** from ${\\mathbf x}i$ to $C_k$ in the image: \n",
    "$$\n",
    "D_a ({\\mathbf x}_i,C_k) = \\min_{\\cal P} \\{ L_a({\\cal P}) ({\\mathbf x}_i,C_k) \\}\n",
    "\\qquad (3)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "L_a({\\cal P}) ({\\mathbf x}_i, C_k) = \\sum_{j=0}^{j=n-1} d_{lab}(p_j,p_{j+1}) + \\alpha \\cdot d_{xy}(p_j,p_{j+1}) + \\beta \\cdot d_g(p_j,p_{j+1})\n",
    "\\qquad (4)\n",
    "$$\n",
    "with $d_{lab}$ and $d_{xy}$ as before. \n",
    "Similarly to $m$ previously, $\\alpha$ quantifies the **relative influences of the spectral proximity _w.r.t_ the spatial one (simply computed as the local Euclidean distance) in the estimation of the perceptual measure**: the higher $\\alpha$, the more spectral closeness is emphasized. \n",
    "In addition, we **incorporate local information about the meaningfull structures present in the image using the cost induced by the gradient structure tensor of the image** $d_g(p_j,p_{j+1})$ [[GDS10]](#GDS10). This way, high gradient pixels act like barriers in the amoeba propagation: the lower the gradient values from $p_j$ to $p_{j+1}$, the higher the probability they belong to the same amoeba. Say it otherwise, the higher $\\beta$, the less probable amoeba superpixels will cross regions of high gradient. Whilst using more information, we will need to deal with the problem of how to weight and combine different kinds of information as  $\\alpha$ and $\\beta$ quantify the relative influences of spectral, spatial and surface proximities in the estimation of the perceptual measure: the higher $\\alpha$ and $\\beta$ are, the less compact the clusters are [[GS08](#GS08), [GS08](#GS09)].  \n",
    "\n",
    "#### Algorithm implementation\n",
    "\n",
    "Given a user-specified amount of superpixels (say $K$), the algorithm first puts, likewise the original SLIC [[ASSLFS12]](#ASSLFS12), some seeds roughly in a lattice structure on the image along with small disturbance in order to avoid the placement on strong intensity boundaries. The seeds serve as initial estimates of the superpixel centers. The location of the centers and shape of each superpixel keep changing in turn as the algorithm runs. \n",
    "\n",
    "<img src=\"algorithm.png\" alt=\"algorithm amoeba superpix\" width=\"700\">\n",
    "\n",
    "In computing geodesic distances in a local $(2S \\times  2S)$  exploration neighborhoods \n",
    "around each cluster center, **the amoeba approach employs priority queues**, whose complexity is $O(w \\log(w))$ in the size of this neighborhood  $w = 4S^2 = 4N/K$ (with: $N$ the total number of pixels). Therefore the estimation of one superpixel area only is $O((N/K) \\log(N/K))$ and the final complexity is $O (N \\log(N/K))$:\n",
    "\\begin{equation}\n",
    " O(\\textsf{cardinal}(\\textsf{superpixels}) \\cdot \\textsf{cost}(\\textsf{superpixel}))  \\\\\n",
    "= O (K \\cdot (N/K) \\cdot \\log(N/K)) \n",
    "= O (N \\cdot \\log(N/K)) \n",
    "\\end{equation}\n",
    "The process that consists in associating pixels with the nearest cluster center and recomputing the cluster center is iteratively repeated until convergence. **By dividing the image into distinct, non-overlapping regions, each containing a number of pixels with some consistent and perceptually meaningful set of properties, a segmentation map is finally formed**.\n",
    "\n",
    "#### Outputs\n",
    "\n",
    "By computing distance along geodesic paths, **the amoeba measure  accounts for both the distance between points and the roughness of the surface**. It penalizes pixels that belong to a different connected geodesic component. Altogether, **the use of amoebas implicitely encodes spatial proximity in the neighbourhood structure and explicitly enforces connectivity, besides compactness and regularity, in the superpixel shapes**. It seamlessly accomodates grayscale or color images.\n",
    "The following properties of the amoeba-based  superpixels are observed _w.r.t_ the five basic principles proposed in [[LSKFDS09]](LSKFDS09):\n",
    "1. **uniform size and coverage**: the algorithm partitions an image into regions that are approximately uniform in size and shape, provided that superpixel size is comparable to the size of the smallest target region; only the superpixels near objects' boundaries deform sharply while the others remain roughly the same during each iteration; in the homogenous region, most resulting superpixels remain roughly like squares;\n",
    "2. **connectivity**: this is automatically ensured using geodesic paths, so that each superpixel represents a simply connected set of pixels;\n",
    "3. **compactness**: each superpixel has a regular shape and size with smooth boundaries which better captures spatially coherent information; in the absence of local edge information, superpixels remain compact;\n",
    "4. **smooth, edge-preserving flow**: the algorithm is not a geometric-flow based formulation, so difficulties that occur in the edge evolution process (such as boundary crossing and collision) are nonexistent;\n",
    "5. **no superpixel overlap**: this is automatically ensured by the algorithm, _i.e._ every pixel is assigned to a single superpixel.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
<<<<<<< HEAD
    "<td><img src=\"img/vierzon.png\" alt=\"input vierzon\" width=\"200\"></td>\n",
    "<td><img src=\"img/vierzon-amoebasuperpix60_1.png\" alt=\"vierzon pix60_1\" width=\"200\"></td>\n",
    "<td><img src=\"img/vierzon-amoebasuperpix200_1.png\" alt=\"vierzon pix200_1\" width=\"200\"></td>\n",
    "<td><img src=\"img/vierzon-amoebasuperpix600_1.png\" alt=\"vierzon pix600_1\" width=\"200\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td width=\"200\"></td>\n",
    "<td><img src=\"img/vierzon-amoebasuperpix60_2.png\" alt=\"vierzon pix60_2\" width=\"200\"></td>\n",
    "<td><img src=\"img/vierzon-amoebasuperpix200_2.png\" alt=\"vierzon pix200_2\" width=\"200\"></td>\n",
    "<td><img src=\"img/vierzon-amoebasuperpix600_2.png\" alt=\"vierzon pix600_2\" width=\"200\"></td>\n",
=======
    "<td><img src=\"img/vierzon.png\" alt=\"input vierzon\" width=\"200\"> </td>\n",
    "<td><img src=\"img/vierzon-amoebasuperpix60_1.png\" alt=\"vierzon pix60_1\" width=\"200\"> </td>\n",
    "<td><img src=\"img/vierzon-amoebasuperpix200_1.png\" alt=\"vierzon pix200_1\" width=\"200\"> </td>\n",
    "<td><img src=\"img/vierzon-amoebasuperpix600_1.png\" alt=\"vierzon pix600_1\" width=\"200\"> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td width=\"200\"></td>\n",
    "<td><img src=\"img/vierzon-amoebasuperpix60_2.png\" alt=\"vierzon pix60_2\" width=\"200\"> </td>\n",
    "<td><img src=\"img/vierzon-amoebasuperpix200_2.png\" alt=\"vierzon pix200_2\" width=\"200\"> </td>\n",
    "<td><img src=\"img/vierzon-amoebasuperpix600_2.png\" alt=\"vierzon pix600_2\" width=\"200\"> </td>\n",
>>>>>>> origin/master
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\" colspan=\"4\">Coarse to fine superpixel representations of a multispectral image. Top left: original image, then from left to right: superpixel regions displayed over the original image for variable inputs $K$: 50, 150 and 500 (top); corresponding linear approximation by spectral averaging over the superpixels (bottom). Note that in this example, the distance vas directly computed over the spectral space (no CIELAB conversion). Image details are progressively retrieved so that local structures are preserved in the finest superpixel representation.</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The resulting superpixels are spatially contiguous, spectrally homogeneous and relatively uniform in area. They can help creating an accurate delineation of objects by simply finding the superpixels which are part of those objects. By varying the amount $K$ of desired superpixels, the amoeba superpixel representation also preserves the relevant spectral and spatial information over scales or sampling resolutions. \n",
    "Instead of the input raw pixel grid, amoeba-based superpixels provide a representation of the image into perceptually meaningful primitives that can be employed in subsequent image analysis and/or interpretation scheme, _e.g._, used as elementary units of any detection, categorization or localization algorithm. They also enable us to compute a piecewise linear approximation of the input images. \n",
    "\n",
    "Besides practical issues (fine tunning, running time, user interaction, etc...), the key related issue is what objects, if any, correspond to the segmented amoeba superpixels. It is however well known that segmentation is an ill-posed problem whose ’correct’ solution is largely dependent on the application, if not completely subjective.\n",
    "\n",
    "**<a name=\"References\"></a>References** \n",
    "\n",
    "* <a name=\"ASSLFS12\"></a>Achanta R., Shaji A., Smith K., Lucchi A., Fua P., and Susstrunk S. (2012): [**SLIC superpixels compared to state-of-the-art superpixel methods**](http://www.kev-smith.com/papers/SMITH_TPAMI12.pdf), _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 34(11):2274–2282, doi:[10.1109/TPAMI.2012.120](http://dx.doi.org/10.1109/TPAMI.2012.120).\n",
    "* <a name=\"BBI08\"></a>Bagon S., Boiman O., and Irani M. (2008): [**What is a good image segment? A unified approach to segment extraction**](http://www.wisdom.weizmann.ac.il/~bagon/pub/good_image_segment.pdf), in _Proc. ECCV_, Lecture Notes in Computer Science, vol. 5305, pp. 30–44, doi:[10.1007/978-3-540-88693-8_3](http://dx.doi.org/10.1007/978-3-540-88693-8_3).\n",
    "* <a name=\"BM06\"></a>Bertelli L. and Manjunath B. (2006): [**Redundancy in all pairs fast marching method**](https://vision.ece.ucsb.edu/sites/vision.ece.ucsb.edu/files/publications/bertelli_redundancy_ICIP06.pdf), in _Proc. IEEE ICIP_, pp. 3033–3036, doi:[10.1109/ICIP.2006.313006](http://dx.doi.org/10.1109/ICIP.2006.313006).\n",
    "* <a name=\"CMT0\"></a>Coeurjolly D., Miguet D., and Tougne L. (2004): [**2D and 3D visibility in discrete geometry: An application to discrete geodesic paths**](http://liris.cnrs.fr/Documents/Liris-1222.pdf), _Pattern Recognition Letters_, \n",
    "25(5):561–570, doi:[10.1016/j.patrec.2003.12.002](http://dx.doi.org/10.1016/j.patrec.2003.12.002).\n",
    "* <a name=\"DP06\"></a>Debayle J. and Pinoli J. (2006): **General adaptive neighborhood image processing**, _Journal of Mathematical Imaging and Vision_; [**Part I: Introduction and theoretical aspects**](https://hal.archives-ouvertes.fr/hal-00128118/document), 25(2):245–266, doi:[10.1007/s10851-006-7451-8](http://dx.doi.org/10.1007/s10851-006-7451-8); [**Part II: practical application examples**](https://hal.inria.fr/hal-00128123/document) 25(2):267–284, doi:[10.1007/s10851-006-7452-7](http://dx.doi.org/10.1007/s10851-006-7452-7).\n",
    "* <a name=\"FH04\"></a>Felzenszwalb P. and Huttenlocher D. (2004): [**Efficient graph-based image segmentation**](http://cs.brown.edu/~pff/papers/seg-ijcv.pdf), _International Journal of Computer Vision_, 59(2):167-181, doi:[10.1023/B:VISI.0000022288.19776.77](http://dx.doi.org/10.1023/B:VISI.0000022288.19776.77).\n",
    "* <a name=\"FVS09\"></a>B. Fulkerson, A. Vedaldi, and S. Soatto (2009): [**Class segmentation and object localization with superpixel neighborhoods**](http://www.vision.cs.ucla.edu/papers/fulkersonVS09.pdf), in _Proc. IEEE ICCV_, pp. 670–677, doi:[10.1109/ICCV.2009.5459175](http://dx.doi.org/10.1109/ICCV.2009.5459175).\n",
    "* <a name=\"GDS10\"></a>Grazzini J., Dillard S., and Soille P. (2010): [**Multichannel image regularisation using anisotropic geodesic filtering**](http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5596008), in _Proc. IEEE ICPR_, pp. 2664-2667, doi:[10.1109/ICPR.2010.653](http://dx.doi.org/10.1109/ICPR.2010.653).\n",
    "* <a name=\"GS09\"></a>Grazzini J. and Soille P. (2009): [**Edge-preserving smoothing using a similarity measure in adaptive geodesic neighbourhoods**](http://www.sciencedirect.com/science/article/pii/S003132030800469X), _Pattern Recognition_, 42(10):2306-2316, doi:[10.1016/j.patcog.2008.11.004](http://dx.doi.org/10.1016/j.patcog.2008.11.004).\n",
    "* <a name=\"GS08\"></a>Grazzini J. and Soille P. (2008): [**Adaptive morphological filters using similarities based on geodesic time**](http://www.springerlink.com/content/f6v62233xqkklq72), in _Proc. DGCI_, Lecture Notes in Computer Science, vol. 4992, pp.519-528, doi:[10.1007/978-3-540-79126-3_46](http://dx.doi.org/10.1007/978-3-540-79126-3_46).\n",
    "* <a name=\"Hanbury08\"></a>Hanbury A. (2008): **How do superpixels affect image segmentation?**, in _Proc. IbCPR_, Lecture Notes in Computer Science, vol. 5197, pp. 178–186, doi:[10.1007/978-3-540-85920-8_22](http://dx.doi.org/10.1007/978-3-540-85920-8_22).\n",
    "* <a name=\"LDM07\"></a>Lerallut R., Decenciere E., and Meyer F. (2007): **Image filtering using morphological amoebas**, _Image and Vision Computing_, 25(4):395–404, doi:[10.1016/j.imavis.2006.04.018](http://dx.doi.org/10.1016/j.imavis.2006.04.018).\n",
    "* <a name=\"LDM07\"></a>Levinshtein A., Stere A., Kutulakos K., Fleet D., Dickinson S., and Siddiqi K. (2009): **TurboPixels: Fast superpixels using geometric flows**, _IEEE Transactions\n",
    "on Pattern Analysis and Machine Intelligence_, 31(12):2290–2297, doi:[10.1109/TPAMI.2009.96](http://dx.doi.org/10.1109/TPAMI.2009.96).\n",
    "* <a name=\"MPWMJ08\"></a>Moore A.P., Prince S., Warrell J., Mohammed U., and Jones G. (2008): [**Superpixel lattices**](http://mplab.ucsd.edu/wordpress/wp-content/uploads/CVPR2008/Conference/data/papers/131.pdf), in _Proc. IEEE CVPR_, pp. 1–8, doi:[10.1109/CVPR.2008.4587471](http://dx.doi.org/10.1109/CVPR.2008.4587471).\n",
    "* <a name=\"RM03\"></a>Ren X. and Malik J. (2003): [**Learning a classification model for segmentation**](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/ren-iccv-03.pdf), in _Proc. ICCV_, pp. 10–17.\n",
    "* <a name=\"SM00\"></a>Shi J. and Malik J. (2000): [**Normalized cuts and image segmentation**](https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf), _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 22(8):888–905, doi:[10.1109/34.868688](http://dx.doi.org/10.1109/34.868688).\n",
    "* <a name=\"Soille08\"></a>Soille P. (2008): **Constrained connectivity for hierarchical image decomposition and simplification**, _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 30(7):1132–1145, doi:[10.1109/TPAMI.2007.70817](http://dx.doi.org/10.1109/TPAMI.2007.70817)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
